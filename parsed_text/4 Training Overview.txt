This chapter provides an overview of Reinforcement Learning from Human Feedback (RLHF) training. It starts by formulating the standard RL problem and then highlights the key differences in the RLHF setup: using a learned reward model instead of an environmental reward function, having no state transitions (treating prompt-completion pairs as single turns), and assigning rewards at the response level. It introduces the simplified RLHF optimization objective for single-turn interactions. The chapter lists common optimization tools used in post-training, including reward modeling, instruction finetuning, rejection sampling, policy gradients, and direct alignment algorithms. It presents a canonical three-step RLHF recipe (Instruction Tuning -> Reward Model Training -> RLHF Training) and contrasts it with modern, multi-round approaches involving more data and stages. Finally, it discusses the importance of regularization in RLHF, particularly KL divergence, to prevent the optimized policy from straying too far from the initial base model, introducing the regularized objective function.4 Training Overview
4.1 Problem Formulation
The optimization of reinforcement learning from human feedback (RLHF) builds on top of the standard RL setup. In RL, an agent takes actions, a, sampled from a policy, π, with respect to the state of the environment, s, to maximize reward, r source: Reinforcement learning: An introduction from 2018. Traditionally, the environment evolves with respect to a transition or dynamics function p(st+1 St, at). Hence, across a finite episode, the goal of an RL agent is to solve the following optimization:
This equation describes the standard reinforcement learning objective, which is to find a policy (π) that maximizes the expected sum of discounted future rewards (γ^t * r(st, at)) obtained by following that policy.
where y is a discount factor from 0 to 1 that balances the desirability of near- versus future-rewards. Multiple methods for optimizing this expression are discussed in Chapter 11.
Figure 2 depicts the standard Reinforcement Learning (RL) loop. An Agent, following its policy (πθ(·)), observes the current state (st) and reward (rt) from the Target Environment. Based on this information, the Agent selects an action (at) which is then applied to the Environment, leading to a new state and reward, thus continuing the loop.
A standard illustration of the RL loop is shown in fig. 2 and how it compares to fig. 3.
4.2 Manipulating the Standard RL Setup
There are multiple core changes from the standard RL setup to that of RLHF:
1. Switching from a reward function to a reward model. In RLHF, a learned model of human preferences, ro(st, at) (or any other classification model) is used instead of an environmental reward function. This gives the designer a substantial increase in the flexibility of the approach and control over the final results.
2. No state transitions exist. In RLHF, the initial states for the domain are prompts sampled from a training dataset and the "action" is the completion to said prompt. During standard practices, this action does not impact the next state and is only scored by the reward model.
3. Response level rewards. Often referred to as a bandit problem, RLHF attribution of reward is done for an entire sequence of actions, composed of multiple generated tokens, rather than in a fine-grained manner.
Given the single-turn nature of the problem, the optimization can be re-written without the time horizon and discount factor (and the reward models):
This equation presents a simplified objective function for RLHF in a single-turn setting. It aims to maximize the expected reward (rθ) assigned by the reward model to a state-action pair (st, at), following the agent's policy (π).
In many ways, the result is that while RLHF is heavily inspired by RL optimizers and problem formulations, the action implementation is very distinct from traditional RL.
Figure 3 illustrates the standard Reinforcement Learning from Human Feedback (RLHF) loop. Training data provides prompts (Si) to the Agent (πθ(·)). The Agent generates completions (Ai). A Reward Model then evaluates these completions, providing a scalar reward (ri). This reward signal is used to update the Agent's policy (θt+1 = θt + α∇J(πθ)) via policy update mechanisms.
4.3 Optimization Tools
In this book, we detail many popular techniques for solving this optimization problem. The popular tools of post-training include:
• Reward modeling (Chapter 7): Where a model is trained to capture the signal from collected preference data and can then output a scalar reward indicating the quality of future text.
• Instruction finetuning (Chapter 9): A prerequisite to RLHF where models are taught the question-answer format used in the majority of language modeling interactions today by imitating preselected examples.
• Rejection sampling (Chapter 10): The most basic RLHF technique where candidate completions for instruction finetuning are filtered by a reward model imitating human preferences.
• Policy gradients (Chapter 11): The reinforcement learning algorithms used in the seminal examples of RLHF to update parameters of a language model with respect to the signal from a reward model.
• Direct alignment algorithms (Chapter 12): Algorithms that directly optimize a policy from pairwise preference data, rather than learning an intermediate reward model to then optimize later.
Modern RLHF-trained models always utilize instruction finetuning followed by a mixture of the other optimization options.
4.4 RLHF Recipe Example
The canonical RLHF recipe circa the release of ChatGPT followed a standard three step post-training recipe where RLHF was the center piece source: Illustrating reinforcement learning from human feedback (RLHF) from 2022 source: Training language models to follow instructions with human feedback from 2022 source: Training a helpful and harmless assistant with reinforcement learning from human feedback from 2022. The three steps taken on top of a "base" language model (the next-token prediction model trained on large-scale web text) was, summarized below in fig. 4:
1. Instruction tuning on ~10K examples: This teaches the model to follow the question-answer format and teaches some basic skills from primarily human-written data.
2. Training a reward model on ~100K pairwise prompts: This model is trained from the instruction-tuned checkpoint and captures the diverse values one wishes to model in their final training. The reward model is the optimization target for RLHF.
3. Training the instruction-tuned model with RLHF on another ~100K prompts: The model is optimized against the reward model with a set of prompts that the model generates over before receiving ratings.
Once RLHF was done, the model was ready to be deployed to users. This recipe is the foundation of modern RLHF, but recipes have evolved substantially to include more stages and more data.
Figure 4 illustrates an early three-stage RLHF process. It starts with a Base Model, which is first fine-tuned using Human Instructions (~10k) to become an SFT (Supervised Fine-Tuning) Model. Separately, Human Preferences (~100k) are used to train a Reward Model. Finally, the SFT Model is optimized using the Reward Model (potentially reusing preference prompts) via PPO (Proximal Policy Optimization) to produce the Aligned Model.
Modern versions of post-training involve many, many more model versions. An example is shown below in fig. 5 where the model undergoes numerous training iterations before convergence.
4.5 Finetuning and Regularization
RLHF is implemented from a strong base model, which induces a need to control the optimization from straying too far from the initial policy. In order to succeed in a finetuning regime, RLHF techniques employ multiple types of regularization to control the optimization. The most common change to the optimization function is to add a distance penalty on
Figure 5 depicts a modern, multi-round post-training process. It begins with a Base Model undergoing initial IFT (Instruction Fine-Tuning) using Human + Synthetic Instructions (~1M+?). This results in an Aligned Model N. Human Preferences (~1M+?) are used to train a Reward Model / LLM Judge. This reward model, along with reused preference prompts and potentially new synthetic completions generated over N rounds, is used in various optimization techniques (DPO, PPO, Rejection Sampling, or multiple optimizations) to further refine the model, yielding Aligned Model N+1. This process can repeat multiple times before resulting in the Final Model.
the difference between the current RLHF policy and the starting point of the optimization:
This equation defines the RLHF objective function incorporating regularization. It aims to maximize the expected reward from the reward model (rθ) while minimizing the KL divergence (DKL) between the current RL policy (πRL) and a reference policy (πref), scaled by a coefficient β. This penalty discourages the learned policy from deviating too much from the initial reference policy.
Within this formulation, a lot of study into RLHF training goes into understanding how to spend a certain "KL budget" as measured by a distance from the initial model. For more details, see Chapter 8 on Regularization.