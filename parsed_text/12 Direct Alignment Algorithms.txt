This chapter introduces Direct Alignment Algorithms (DAAs), focusing primarily on Direct Preference Optimization (DPO) as a method to align language models with human preferences without explicit reward modeling or reinforcement learning. DPO works by directly optimizing a policy to maximize the probability of preferred responses over rejected ones, relative to a reference model, using a specific loss function derived from the Bradley-Terry model. The chapter details the mathematical derivation of the DPO loss function, its implicit reward formulation, and its gradient. It discusses the core intuition that DPO implicitly learns a reward model. Numerical concerns and weaknesses are addressed, such as DPO treating all preference pairs equally and the phenomenon of 'preference displacement' where probabilities of both chosen and rejected responses decrease, albeit at different rates. Several alternatives and variants like REBEL, cDPO, IPO, ODPO, ORPO, SimPO, Cal-DPO, AlphaPO, Online DPO, and D2PO are mentioned, designed to mitigate these issues or improve efficiency. Implementation considerations, including the static nature of the KL constraint (β) and log-probability caching strategies, are explained. Finally, the chapter compares DAAs (offline) with traditional RL methods (online), acknowledging reports of RL's performance edge but highlighting DAA's simplicity and continued relevance, while suggesting a potential long-term shift back towards RL for complex tasks like reasoning.12 Direct Alignment Algorithms
Direct Alignment Algorithms (DAAs) allow one to update models to solve the same RLHF objective without ever training an intermediate reward model or using reinforcement learning optimizers. The most prominent DAA and one that catalyzed an entire academic movement of aligning language models is Direct Preference Optimization (DPO) [source: "Direct preference optimization: Your language model is secretly a reward model," Advances in Neural Information Processing Systems, vol. 36, 2024.]. At its core, DPO is using gradient ascent to solve the same constrained RLHF objective. Since its release in May of 2023, after a brief delay where the community figured out the right data and hyperparameters to use DPO with (specifically, surprisingly low learning rates), many popular models have used DPO or its variants, from Zephyr-ẞ kickstarting it in October of 2023 [source: "Zephyr: Direct distillation of LM alignment," in First conference on language modeling, 2024.], Llama 3 Instruct [source: "The llama 3 herd of models," arXiv preprint arXiv:2407.21783, 2024.], Tülu 2 [source: "Camels in a changing climate: Enhancing lm adaptation with tulu 2," arXiv preprint arXiv:2311.10702, 2023.] and 3 [source: "T ULU 3: Pushing frontiers in open language model post-training," arXiv preprint arXiv:2411.15124, 2024.], Nemotron 4 340B [source: "Nemotron-4 340B technical report," arXiv preprint arXiv:2406.11704, 2024.], and others. Technically, Sequence Likelihood Calibration (SLiC-HF) was released first [source: "Slic-hf: Sequence likelihood calibration with human feedback,” arXiv preprint arXiv:2305.10425, 2023.], but it did not catch on due to a combination of luck and effectiveness.
The most impactful part of DPO and DAAs is lowering the barrier of entry to experimenting with language model post-training.

12.1 Direct Preference Optimization (DPO)
Here we explain intuitions for how it works and re-derive the core equations fully.

12.1.1 How DPO Works
DPO at a surface level is directly optimizing a policy to solve the RLHF objective. The loss function for this, which we will revisit below in the derivations, is a pairwise relationship of log-probabilities. The loss function derived from a Bradley-Terry reward model follows:
This equation presents the DPO loss function, which aims to maximize the likelihood of preferred responses (yc) over rejected responses (yr) relative to a reference model (πref), weighted by a factor β. It's calculated as the negative expectation over the preference data (D) of the log-sigmoid of the difference between the log-probability ratios (policy vs. reference) for the chosen and rejected responses.
This relies on the implicit reward for DPO training that replaces using an external reward model, which is a log-ratio of probabilities:
This equation defines the implicit reward (r) in DPO as the β-scaled log-ratio of the policy's probability (πθ) to the reference model's probability (πref) for a given response (y) to a prompt (x).
This comes from deriving the Bradley-Terry reward with respect to an optimal policy (shown in eq. 80), as shown in the Bradley-Terry model section. Essentially, the implicit reward model shows "the probability of human preference data in terms of the optimal policy rather than the reward model."
Let us consider the loss shown in eq. 65. The learning process is decreasing the loss. Here, the loss will be lower when the log-ratio of the chosen response is bigger than the log-ratio of the rejected response (normalized by the reference model). In practice, this is a sum of log-probabilities of the model across the sequence of tokens in the data presented. Hence, DPO is increasing the delta in probabilities between the chosen and rejected responses.
With the reward in eq. 66, we can write the gradient of the loss to further interpret what is going on:
This equation shows the gradient of the DPO loss function. It indicates that the gradient update is weighted by a sigmoid function (σ) of the difference in implicit rewards between rejected and chosen responses. The update increases the log probability of the chosen response (yc) and decreases the log probability of the rejected response (yr). The weight β balances preference alignment against KL divergence from the reference model.
Here, the gradient solves the above objective by doing the following:

*   The first term within the sigmoid function, σ(·), creates a weight of the parameter update from 0 to 1 that is higher when the reward estimate is incorrect. When the rejected sample is preferred over the chosen, the weight update should be larger!
*   Second, the terms in the inner brackets [.] increases the likelihood of the chosen response yc and decreases the likelihood of the rejected yr.
*   These terms are weighted by ß, which controls how the update balances ordering the completions correctly relative to the KL distance.

The core intuition is that DPO is "fitting an implicit reward model whose corresponding optimal policy can be extracted in a closed form" (thanks to gradient ascent and our ML tools). What is often misunderstood is that DPO is learning a reward model at its core, hence the subtitle of the paper Your Language Model is Secretly a Reward Model. It is easy to confuse this with the DPO objective training a policy directly, hence studying the derivations below are good for a complete understanding.
With the implicit reward model learning, DPO is generating an optimal solution to the RLHF objective given the data in the dataset and the specific KL constraint in the objective β. Here, DPO solves for the exact policy given a specific KL distance because the generations are not online as in policy gradient algorithms a core difference from the RL methods for preference tuning. In many ways, this makes the ẞ value easier to tune with DPO relative to online RL methods, but crucially and intuitively the optimal value depends on the model being trained and the data training it.
At each batch of preference data, composed of many pairs of completions Ychosen ➤ Yrejected, DPO takes gradient steps directly towards the optimal solution. It is far simpler than policy gradient methods.

12.1.2 DPO Derivation
The DPO derivation takes two primary parts. First, the authors show the form of the policy that optimally solved the RLHF objective used throughout this book. Next, they show how to arrive at that solution from pairwise preference data (i.e. a Bradley Terry model).

12.1.2.1 1. Deriving the Optimal RLHF Solution To start, we should consider the RLHF optimization objective once again, here indicating we wish to maximize this quantity:
This equation restates the standard RLHF objective, aiming to maximize the expected reward under the learned policy (π) while penalizing its KL divergence from a reference policy (πref), scaled by β.
First, let us expand the definition of KL-divergence,
This equation expands the RLHF objective from Eq. 68 by substituting the definition of KL divergence, showing the objective is to maximize the expected difference between the reward r(x, y) and the β-scaled log-ratio of the policy π to the reference policy πref.
Figure 13 is a meme illustrating the perceived simplicity of DPO compared to traditional RLHF methods like PPO. On one side (labeled DPO), a simple character represents performing 'gradient descent on good stuff, gradient ascent on bad stuff'. On the other side (labeled PPO and RL), a more complex, stressed character represents dealing with 'value funcs and on-policy RL and math'. The meme humorously suggests DPO is a more straightforward approach to learning from human feedback.
Next, pull the negative sign out of the difference in brackets. To do this, split it into two terms:
This equation manipulates the RLHF objective (from eq. 69) by pulling the negative sign out of the KL divergence term.
Then, remove the factor of -1 and β,
This equation further simplifies the objective by removing the factor of -1 and β, turning the maximization into a minimization problem involving the negative expected reward and the positive expected log-probability ratio.
Divide by β and recombine:
This equation recombines terms after dividing by β, showing the minimization objective involves the expected value of the log-probability ratio minus (1/β) times the reward.
Next, we must introduce a partition function, Z(x):
This equation introduces the partition function Z(x), defined as the sum over all possible responses (y) of the reference policy's probability for that response, weighted by the exponentiated reward (scaled by 1/β). It acts as a normalization factor.
The partition function acts as a normalization factor over the reference policy, summing over all possible responses y to a prompt x. With this substituted in, we obtain our intermediate transformation:
This equation substitutes the partition function Z(x) into the optimization objective (from eq. 72), expressing the objective in terms of the log-ratio of the policy probability π(y|x) to the exponentiated reward term normalized by Z(x), minus log Z(x).
To see how this is obtained, consider the internal part of the optimization in brackets of eq. 72:
This equation isolates the term inside the expectation in eq. 72, which is the log-ratio of the policy π to the reference πref minus the scaled reward r(x,y)/β.
Then, add log Z(x) - log Z(x) to both sides:
This equation adds and subtracts log Z(x) to the expression from eq. 75, a common algebraic trick used to rearrange terms.
Then, we group the terms:
This equation groups the terms from eq. 76, combining the log policy ratio with log Z(x) and separating out log Z(x) and the scaled reward.
With log(x) + log(y) = log(xy) (and moving Z to the denominator), we get:
This equation combines the log terms from eq. 77 using log properties, resulting in the logarithm of the policy probability π divided by the product of Z(x) and the reference probability πref, minus log Z(x) and the scaled reward.
Next, we expand (1/β)r(x, y) to log exp((1/β)r(x, y)) and do the same operation to get eq. 74. With this optimization form, we need to actually solve for the optimal policy π*. To do so, let us consider the above optimization as a KL distance:
This equation rewrites the optimization objective (from eq. 74, derived via eq. 72 & 78) as minimizing the expected KL divergence between the learned policy π(y|x) and a target distribution defined by the reference policy πref, the reward r(x,y), β, and the partition function Z(x). The log Z(x) term is outside the KL divergence.
Since the partition function Z(x) does not depend on the final answer, we can ignore it. This leaves us with just the KL distance between our policy we are learning and a form relating the partition, β, reward, and reference policy. The Gibb's inequality tells this is minimized at a distance of 0, only when the two quantities are equal! Hence, we get an optimal policy:
Based on Gibb's inequality, the KL divergence in eq. 79 is minimized (at 0) when the two distributions are equal. This equation defines the optimal policy π*(y|x) derived from this minimization, showing it's proportional to the reference policy πref(y|x) times the exponentiated reward scaled by 1/β, normalized by the partition function Z(x).

12.1.2.2 2. Deriving DPO Objective for Bradley Terry Models To start, recall from Chapter 7 on Reward Modeling and Chapter 6 on Preference Data that a Bradley-Terry model of human preferences is formed as:
This equation presents the Bradley-Terry model for human preferences, stating the probability P* that response y1 is preferred over y2 given prompt x is the ratio of the exponentiated reward of y1 to the sum of the exponentiated rewards of y1 and y2.
By manipulating eq. 80 by taking the logarithm of both sides and performing some algebra, one can obtain the DPO reward as follows:
This equation expresses the optimal reward r*(x, y) in terms of the optimal policy π*(y|x), the reference policy πref(y|x), the scaling factor β, and the partition function Z(x). It's derived by taking the logarithm of eq. 80.
We then can substitute the reward into the Bradley-Terry equation shown in eq. 81 to obtain:
This equation substitutes the DPO reward definition (eq. 82) into the Bradley-Terry preference model (eq. 81). The probability of preferring y1 over y2 is now expressed in terms of the log-ratios of the optimal policy to the reference policy for both y1 and y2, plus the log partition function term, all exponentiated.
By decomposing the exponential expressions from e^(a+b) to e^a*e^b and then cancelling out the terms e^(β log Z(x)), this simplifies to:
This equation simplifies eq. 83 by using the property e^(a+b) = e^a * e^b and canceling out the e^(β log Z(x)) terms from the numerator and denominator. The preference probability is now expressed purely in terms of exponentiated β-scaled log-ratios of the optimal policy π* to the reference policy πref for y1 and y2.
Then, multiply the numerator and denominator by exp(-βlog (π*(y1|x)/πref(y1|x))) to obtain:
This equation further simplifies the Bradley-Terry preference probability (from eq. 84) by multiplying the numerator and denominator by exp(-β log(π*(y1|x)/πref(y1|x))), resulting in a form resembling the logistic sigmoid function.
Finally, with the definition of a sigmoid function as σ(x) = 1/(1+e^-x), we obtain:
This equation shows the final form of the Bradley-Terry preference probability P*(y1 > y2 | x) expressed using the logistic sigmoid function σ applied to the difference between the β-scaled log-ratios (policy vs. reference) for y1 and y2. This links directly back to the DPO loss function (eq. 65).
This is the loss function for DPO, as shown in eq. 65. The DPO paper has an additional derivation for the objective under a Plackett-Luce Model, which is far less used in practice [source: "Direct preference optimization: Your language model is secretly a reward model," Advances in Neural Information Processing Systems, vol. 36, 2024.].

12.1.2.3 3. Deriving the Bradley Terry DPO Gradient We used the DPO gradient shown in eq. 67 to explain intuitions for how the model learns. To derive this, we must take the gradient of eq. 86 with respect to the model parameters.
This equation restates the gradient of the DPO loss (initially shown in eq. 67) using the final loss form derived from the Bradley-Terry model (eq. 86). It involves taking the gradient of the negative expectation of the log-sigmoid function applied to the difference in β-scaled log-ratios.
To start, this can be rewritten. We know that the derivative of a sigmoid function d/dx σ(x) = σ(x)(1 – σ(x)), the derivative of logarithm d/dx log x = 1/x, and properties of sigmoid σ(-x) = 1 – σ(x), so we can reformat the above equation.
First, define the expression inside the sigmoid as u = β log(πθ(yc|x)/πref(yc|x)) - β log(πθ(yr|x)/πref(yr|x)). Then, we have
This equation starts the derivation of the gradient using the chain rule, defining 'u' as the term inside the sigmoid function and expressing the gradient in terms of the derivative of the sigmoid (σ'(u)/σ(u)) times the gradient of u (∇θu).
Expanding this and using the above expressions for sigmoid and logarithms results in the gradient introduced earlier:
This equation presents the final derived gradient of the DPO loss function, matching the form shown earlier in eq. 67. It expands the terms from eq. 88 using properties of sigmoid and logarithm derivatives, showing the gradient encourages increasing the probability of the chosen response (yc) and decreasing the probability of the rejected response (yr), weighted by the sigmoid term (which depends on the difference in implicit rewards) and β.

12.2 Numerical Concerns, Weaknesses, and Alternatives
Many variants of the DPO algorithm have been proposed to address weaknesses of DPO. For example, without rollouts where a reward model can rate generations, DPO treats every pair of preference data with equal weight. In reality, as seen in Chapter 6 on Preference Data, there are many ways of capturing preference data with a richer label than binary. Multiple algorithms have been proposed to re-balance the optimization away from treating each pair equally.

*   REgression to RElative REward Based RL (REBEL) adds signal from a reward model, as a margin between chosen and rejected responses, rather than solely the pairwise preference data to more accurately solve the RLHF problem [source: "Rebel: Reinforcement learning via regressing relative rewards," arXiv preprint arXiv:2404.16767, 2024.].
*   Conservative DPO (cDPO) and Identity Preference Optimization (IPO) address the overfitting by assuming noise in the preference data. CDPO assumes N percent of the data is incorrectly labelled [source: "Direct preference optimization: Your language model is secretly a reward model," Advances in Neural Information Processing Systems, vol. 36, 2024.] and IPO changes the optimization to soften probability of preference rather than optimize directly from a label [source: "A general theoretical paradigm to understand learning from human preferences,” in International conference on artificial intelligence and statistics, PMLR, 2024, pp. 4447-4455.]. Practically, IPO changes the preference probability to a nonlinear function, moving away from the Bradley-Terry assumption, with ψ(q) = log(q / (1-q)).
*   DPO with an offset (ODPO) "requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value" [source: "Direct preference optimization with an offset," arXiv preprint arXiv:2402.10571, 2024.] – do not treat every data pair equally, but this can come at the cost of a more difficult labeling environment.

Some variants to DPO attempt to either improve the learning signal by making small changes to the loss or make the application more efficient by reducing memory usage.

*   Odds Ratio Policy Optimization (ORPO) directly updates the policy model with a pull towards the chosen response, similar to the instruction finetuning loss, with a small penalty on the chosen response [source: "Reference-free monolithic preference optimization with odds ratio," arXiv e-prints, pp. arXiv-2403, 2024.]. This change of loss function removes the need for a reference model, simplifying the setup. The best way to view ORPO is DPO inspired, rather than a DPO derivative.
*   Simple Preference Optimization SimPO makes a minor change to the DPO optimization, by averaging the log-probabilities rather than summing them (SimPO) or adding length normalization, to improve performance [source: "Simpo: Simple preference optimization with a reference-free reward," Advances in Neural Information Processing Systems, vol. 37, pp. 124198-124235, 2025.].
Figure 14 sketches the effect of DPO training on the log-probabilities of chosen and rejected responses over training steps. The y-axis represents response log-probabilities, and the x-axis represents training steps. Two lines are shown: one for 'CHOSEN' responses and one for 'REJECTED' responses. Both lines trend downwards, indicating that DPO tends to reduce the log-probabilities of both types of responses. However, the line for rejected responses decreases more steeply than the line for chosen responses. An arrow indicates the 'DPO Margin (Higher is better)' on the left plot, showing this margin increases over training. The difference (margin) between the chosen and rejected log-probabilities increases, which is the direct optimization target. The right plot illustrates this displacement, showing the gap widening as both chosen (blue line) and rejected (red line) log-probabilities decrease, but the rejected decrease more.
One of the core issues apparent in DPO is that the optimization drives only to increase the margin between the probability of the chosen and rejected responses. Numerically, the model reduces the probability of both the chosen and rejected responses, but the rejected response is reduced by a greater extent as shown in fig. 14. Intuitively, it is not clear how this generalizes, but work has posited that it increases the probability of unaddressed for behaviors [source: "Unintentional unalignment: Likelihood displacement in direct preference optimization," arXiv preprint arXiv:2410.08847, 2024.] [source: "Learning dynamics of llm finetuning," arXiv preprint arXiv:2407.10490, 2024.]. Simple methods such as Cal-DPO [source: "Cal-dpo: Calibrated direct preference optimization for language model alignment," arXiv preprint arXiv:2412.14516, 2024.], which adjusts the optimization process, and AlphaPO [source: "AlphaPO-reward shape matters for LLM alignment," arXiv preprint arXiv:2501.03884, 2025.], which modifies the reward shape—mitigate this preference displacement. In practice, the exact impact of this is not well known, but points are a potential reason why online methods can outperform vanilla DPO.
The largest other reason that is posited for DPO-like methods to have a lower ceiling on performance than online (RL based) RLHF methods is that the training signal comes from completions from previous or other models. Online variants that sample generations from the model, e.g. Online DPO [source: “Direct language model alignment from online ai feedback,” arXiv preprint arXiv:2402.04792, 2024.], even with regular reward model relabelling of newly created creations Discriminator-Guided DPO (D2PO) [source: "D2po: Discriminator-guided dpo with response evaluation models," arXiv preprint arXiv:2405.01511, 2024.], alleviate these by generating new completions for the prompt and incorporating a preference signal at training time.
There is a long list of other DAA variants, such as Direct Nash Optimization (DNO) [source: "Direct nash optimization: Teaching language models to self-improve with general preferences," arXiv preprint arXiv:2404.03715, 2024.] or Binary Classifier Optimization (BCO) [source: "Binary classifier optimization for large language model alignment," arXiv preprint arXiv:2404.04656, 2024.], but the choice of algorithm is far less important than the initial model and the data used [source: "T ULU 3: Pushing frontiers in open language model post-training," arXiv preprint arXiv:2411.15124, 2024.] [source: "Rainbowpo: A unified framework for combining improvements in preference optimization," arXiv preprint arXiv:2410.04203, 2024.] [source: "The differences between direct alignment algorithms are a blur," arXiv preprint arXiv:2502.01237, 2025.].

12.3 Implementation Considerations
DAAs such as DPO are implemented very differently than policy gradient optimizers. The DPO loss, taken from the original implementation, largely can be summarized as follows [source: "Direct preference optimization: Your language model is secretly a reward model," Advances in Neural Information Processing Systems, vol. 36, 2024.]:
pi_logratios = policy_chosen_logps - policy_rejected_logps
ref_logratios = reference_chosen_logps - reference_rejected_logps
logits = pi_logratios - ref_logratios # also known as h_{\pi_\theta}^{y_w,y_l}
losses = -F.logsigmoid(beta * logits)
chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()
rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()

This can be used in standard language model training stacks as this information is already collated during the forward pass of a model (with the addition of a reference model).
In most ways, this is simpler and an quality of life improvement, but also they offer a different set of considerations.

1.  KL distance is static: In DPO and other algorithms, the KL distance is set explicitly by the ẞ parameter that balances the distance penalty to the optimization. This is due to the fact that DPO takes gradient steps towards the optimal solution to the RLHF objective given the data it steps exactly to the solution set by the ẞ term. On the other hand, RL based optimizers take steps based on the batch and recent data.
2.  Caching log-probabilities: Simple implementations of DPO do the forward passes for the policy model and reference models at the same time for conveniences with respect to the loss function. Though, this doubles the memory used and results in increased GPU usage. To avoid this, one can compute the log-probabilities of the reference model over the training dataset first, then reference it when computing the loss and updating the parameters per batch, reducing the peak memory usage by 50%.

12.4 DAAs vs. RL: Online vs. Offline Data
Broadly, the argument boils down to one question: Do we need the inner workings of reinforcement learning, with value functions, policy gradients, and all, to align language models with RLHF? This, like most questions phrased this way, is overly simplistic. Of course, both methods are well-established, but it is important to illustrate where the fundamental differences and performance manifolds lie.
Multiple reports have concluded that policy-gradient based and RL methods outperform DPO and its variants. The arguments take different forms, from training models with different algorithms but controlled data[source: “Unpacking DPO and PPO: Disentangling best practices for learning from preference feedback," arXiv preprint arXiv:2406.09279, 2024.] [source: "Is dpo superior to ppo for llm alignment? A comprehensive study," arXiv preprint arXiv:2404.10719, 2024.] or studying the role of on-policy data within the RL optimization loop [source: “Preference fine-tuning of llms should leverage suboptimal, on-policy data," arXiv preprint arXiv:2404.14367, 2024.]. In all of these cases, DPO algorithms are a hair behind.
Even with this performance delta, DAA are still used extensively in leading models due to its simplicity. DAAs provide a controlled environment where iterations on training data and other configurations can be made rapidly, and given that data is often far more important than algorithms, using DPO can be fine.
With the emergence of reasoning models that are primarily trained with RL, further investment will return to using RL for preference-tuning, which in the long-term will improve the robustness of RL infrastructure and cement this margin between DAAs and RL for optimizing from human feedback.