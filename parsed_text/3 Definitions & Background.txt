This chapter introduces fundamental definitions and background concepts crucial for understanding Reinforcement Learning from Human Feedback (RLHF). It begins with a Language Modeling Overview, explaining how modern models like Transformers use autoregression and self-attention to predict token sequences, trained by minimizing Negative Log-Likelihood (NLL) loss. Key Machine Learning definitions include Kullback-Leibler (KL) divergence, a measure of difference between probability distributions. Natural Language Processing (NLP) definitions cover essential terms like Prompt, Completion (including Chosen and Rejected variants), Preference Relation, and Policy (as a distribution over completions). Reinforcement Learning (RL) definitions detail concepts such as Reward, Action, State, Trajectory, Trajectory Distribution, Policy (as an action-selection strategy), Discount Factor, Value Function (V), Q-Function (Q), and Advantage Function (A). It also discusses policy-conditioned values and the primary RL goal of maximizing expected cumulative reward, including the finite-horizon case. The concept of 'on-policy' data in the RLHF context is clarified. A specific RLHF definition provided is the Reference Model (πref), used for regularization. Finally, an Extended Glossary defines Synthetic Data, Distillation, (Teacher-student) Knowledge Distillation (including its loss function), In-context Learning (ICL), and Chain of Thought (CoT) prompting.3 Definitions & Background
This chapter includes all the definitions, symbols, and operations frequently used in the RLHF process and with a quick overview of language models (the common optimization target of this book).
3.1 Language Modeling Overview
The majority of modern language models are trained to learn the joint probability distribution of sequences of tokens (words, subwords, or characters) in a autoregressive manner. Autoregression simply means that each next prediction depends on the previous entities in the sequence. Given a sequence of tokens x = (x1,x2,...,xT), the model factorizes the probability of the entire sequence into a product of conditional distributions:
The formula Pθ(x) represents the probability of a sequence x under model θ as the product, from t=1 to T, of the conditional probabilities Pθ(xt | x1, ..., xt−1), meaning the probability of the t-th token given all preceding tokens.
In order to fit a model that accurately predicts this, the goal is often to maximize the likelihood of the training data as predicted by the current model. To do so we can minimize a negative log-likelihood (NLL) loss:
The formula LLM(θ) represents the language model loss for parameters θ as the negative expectation over data D of the sum, from t=1 to T, of the log probability of the true token xt given the preceding context x<t, according to the model Pθ.
In practice, one uses a cross-entropy loss with respect to each next-token prediction, computed by comparing the true token in a sequence to what was predicted by the model.
Implementing a language model can take many forms. Modern LMs, including ChatGPT, Claude, Gemini, etc., most often use decoder-only Transformers (source: "Attention is all you need," in Neural information processing systems, 2017.). The core innovation of the Transformer was heavily utilizing the self-attention (source: "Neural machine translation by jointly learning to align and translate," CoRR, vol. abs/1409.0473, 2014,) mechanism to allow the model to directly attend to concepts in context and learn complex mappings. Throughout this book, particularly when covering reward models in Chapter 7, we will discuss adding new heads or modifying a language modeling (LM) head of the transformer. The LM head is a final linear projection layer that maps from the models internal embedding space to the tokenizer space (a.k.a. vocabulary). Different heads can be used to re-use the internals of the model and fine-tune it to output differently shaped quantities.
3.2 ML Definitions
• Kullback-Leibler (KL) divergence (DKL(P||Q)), also known as KL divergence, is a measure of the difference between two probability distributions. For discrete probability distributions P and Q defined on the same probability space X, the KL distance from Q to P is defined as:
The formula DKL(P||Q) defines the Kullback-Leibler divergence from distribution Q to P as the sum over all x in the space X, of P(x) times the logarithm of the ratio P(x)/Q(x).
3.3 NLP Definitions
• Prompt (x): The input text given to a language model to generate a response or completion.
• Completion (y): The output text generated by a language model in response to a prompt. Often the completion is denoted as y|x.
• Chosen Completion (yc): The completion that is selected or preferred over other alternatives, often denoted as ychosen.
• Rejected Completion (yr): The disfavored completion in a pairwise setting.
• Preference Relation (>): A symbol indicating that one completion is preferred over another, e.g., ychosen > yrejected.
• Policy (π): A probability distribution over possible completions, parameterized by θ: πθ(y|x).
3.4 RL Definitions
• Reward (r): A scalar value indicating the desirability of an action or state, typically denoted as r.
• Action (a): A decision or move made by an agent in an environment, often represented as a ∈ A, where A is the set of possible actions.
• State (s): The current configuration or situation of the environment, usually denoted as s ∈ S, where S is the state space.
• Trajectory (τ): A trajectory τ is a sequence of states, actions, and rewards experienced by an agent: τ = (s0, a0, r0, s1, a1, r1, ..., sT, aT, rT).
• Trajectory Distribution (P(τ|π)): The probability of a trajectory under policy π is defined as the product of the initial state probability p(s0) and the product over time t=0 to T-1 of the policy's probability of taking action at given state st (π(at|st)) and the environment's transition probability p(st+1|st, at).
• Policy (π), also called the policy model in RLHF: In RL, a policy is a strategy or rule that the agent follows to decide which action to take in a given state: π(a|s).
• Discount Factor (γ): A scalar 0 ≤ γ ≤ 1 that exponentially down-weights future rewards in the return, trading off immediacy versus long-term gain and guaranteeing convergence for infinite-horizon sums. Sometimes discounting is not used, which is equivalent to γ = 1.
• Value Function (V): A function that estimates the expected cumulative reward from a given state: V(s) represents the expected sum of discounted future rewards starting from state s, following a given policy.
• Q-Function (Q): A function that estimates the expected cumulative reward from taking a specific action in a given state: Q(s, a) represents the expected sum of discounted future rewards starting from state s, taking action a, and following a given policy thereafter.
• Advantage Function (A): The advantage function A(s, a) quantifies the relative benefit of taking action a in state s compared to the average action. It's defined as A(s, a) = Q(s, a) – V(s). Advantage functions (and value functions) can depend on a specific policy, Aπ(s, a).
• Policy-conditioned Values ([]π): Across RL derivations and implementations, a crucial component of the theory and practice is collecting data or values conditioned on a specific policy. Throughout this book we will switch between the simpler notation of value functions et al. (V, A, Q, G) and their specific policy-conditioned values (Vπ, Aπ, Qπ). Crucial is also in the expected value computation is sampling from data dπ that is conditioned on a specific policy, dπ.
• Expectation of Reward Optimization: The primary goal in RL, which involves maximizing the expected cumulative reward:
The formula shows the objective is to maximize, over policy parameters θ, the expected sum of discounted rewards (γ^t * rt) over an infinite horizon (t=0 to ∞), where states s are sampled from the state distribution ρπ induced by policy πθ, and actions a are sampled from the policy πθ itself.
where ρπ is the state distribution under policy π, and γ is the discount factor.
• Finite Horizon Reward (J(πθ)): The expected finite-horizon discounted return of the policy πθ, parameterized by θ is defined as: J(πθ) represents the expected total discounted reward (sum of γ^t * rt from t=0 to T) for trajectories τ sampled according to the policy πθ, where T is the finite horizon.
• On-policy: In RLHF, particularly in the debate between RL and Direct Alignment Algorithms, the discussion of on-policy data is common. In the RL literature, on-policy means that the data is generated exactly by the current form of the agent, but in the general preference-tuning literature, on-policy is expanded to mean generations from that edition of model e.g. a instruction tuned checkpoint before running any preference fine-tuning. In this context, off-policy could be data generated by any other language model being used in post-training.
3.5 RLHF Only Definitions
• Reference Model (πref): This is a saved set of parameters used in RLHF where outputs of it are used to regularize the optimization.
3.6 Extended Glossary
• Synthetic Data: This is any training data for an AI model that is the output from another AI system. This could be anything from text generated from a open-ended prompt of a model to a model re-writing existing content.
• Distillation: Distillation is a general set of practices in training AI models where a model is trained on the outputs of a stronger model. This is a type of synthetic data known to make strong, smaller models. Most models make the rules around distillation clear through either the license, for open weight models, or the terms of service, for models accessible only via API. The term distillation is now overloaded with a specific technical definition from the ML literature.
• (Teacher-student) Knowledge Distillation: Knowledge distillation from a specific teacher to student model is a specific type of distillation above and where the term originated (source: "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, 2015.). It is a specific deep learning method where a neural network loss is modified to learn from the log-probabilites of the teacher model over multiple potential tokens/logits, instead of learning directly from a chosen output. An example of a modern series of models trained with Knowledge Distillation is Gemma 2 (source: “Gemma 2: Improving open language models at a practical size," arXiv preprint arXiv:2408.00118, 2024.) or Gemma 3. For a language modeling setup, the next-token loss function can be modified as follows (source: "On-policy distillation of language models: Learning from self-generated mistakes," in The twelfth international conference on learning representations, 2024.), where the student model Pθ learns from the teacher distribution P*:
The formula LKD(θ) defines the Knowledge Distillation loss for parameters θ as the negative expectation over data D of the sum, from t=1 to T, of the teacher distribution's probability P*(xt|x<t) multiplied by the log probability assigned by the student model Pθ(xt|x<t).
• In-context Learning (ICL): In-context here refers to any information within the context window of the language model. Usually, this is information added to the prompt. The simplest form of in-context learning is adding examples of a similar form before the prompt. Advanced versions can learn which information to include for a specific use-case.
• Chain of Thought (CoT): Chain of thought is a specific behavior of language models where they are steered towards a behavior that breaks down a problem in a step by step form. The original version of this was through the prompt "Let's think step by step" (source: “Chain-of-thought prompting elicits reasoning in large language models," Advances in neural information processing systems, vol. 35, pp. 24824-24837, 2022.).