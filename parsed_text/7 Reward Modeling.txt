This chapter delves into Reward Modeling (RM), a core component of Reinforcement Learning from Human Feedback (RLHF). It explains that RMs act as proxies for rewards, predicting the likelihood that one piece of text is preferred over another based on human comparison data. The standard training approach utilizes the Bradley-Terry model, deriving a loss function (typically negative log-likelihood of the sigmoid of reward differences) optimized to reflect preferences. The chapter details the common architecture (LM with a classification head) and provides an implementation example. It then explores variants like Preference Margin Loss (using preference magnitude), balancing loss for multiple comparisons per prompt, and K-wise loss functions (e.g., Plackett-Luce). The text contrasts standard RMs with Outcome Reward Models (ORMs), which predict per-token correctness for tasks with verifiable answers, and Process Reward Models (PRMs), which provide feedback at each step of a reasoning process. A comparative table summarizes their predictions, training methods, and structures. The chapter also introduces Generative Reward Modeling, where LLMs themselves act as judges ('LLM-as-a-judge'), and discusses the burgeoning field of RM evaluation benchmarks (like RewardBench, M-RewardBench, PRM Bench) and recent advancements in RM training techniques, including new datasets, scaling techniques, and debiasing methods.7 Reward Modeling
Reward models are core to the modern approach to RLHF. Reward models broadly have been used extensively in reinforcement learning research as a proxy for environment rewards (source: Reinforcement learning: An introduction, 2018). The practice is closely related to inverse reinforcement learning, where the problem is to approximate an agent's reward function given trajectories of behavior (source: Algorithms for inverse reinforcement learning, 2000), and other areas of deep reinforcement learning. Reward models were proposed, in their modern form, as a tool for studying the value alignment problem (source: Scalable agent alignment via reward modeling: A research direction, 2018).
The most common reward model predicts the probability that a piece of text was close to a "preferred" piece of text from the training comparisons. Later in this section we also compare these to Outcome Reward Models (ORMs) that predict the probability and a completion results in a correct answer or a Process Reward Model (PRM) that assigns a score to each step in reasoning. When not indicated, the reward models mentioned are those predicting preference between text.

7.1 Training Reward Models
There are two popular expressions for how to train a standard reward model for RLHF - they are numerically equivalent. The canonical implementation is derived from the Bradley-Terry model of preference (source: Rank analysis of incomplete block designs: I. The method of paired comparisons, 1952). A Bradley-Terry model of preferences measures the probability that the pairwise comparison for two events drawn from the same distribution, say i and j, satisfy the following relation, i > j:
This formula defines the probability of preferring item 'i' over item 'j' as the ratio of the score of 'i' to the sum of the scores of 'i' and 'j'.
To train a reward model, we must formulate a loss function that satisfies the above relation. The first structure applied is to convert a language model into a model that outputs a scalar value, often in the form of a single classification probability logit. Thus, we can take the score of this model with two samples, the i and j above are now completions, y₁ and y2, to one prompt, x and score both of them with respect to the above model, r_theta.
The probability of success for a given reward model in a pairwise comparison, becomes:
This formula calculates the probability that completion y1 is preferred over y2 based on the exponential of their respective reward scores, derived from the Bradley-Terry model.
Then, by maximizing the log-likelihood of the above function (or alternatively minimizing the negative log-likelihood), we can arrive at the loss function to train a reward model:
This set of equations derives the optimal parameters theta for the reward model by maximizing the probability of the preferred completion (yw) being ranked higher than the less preferred completion (yl). It simplifies to maximizing the sigmoid function applied to the difference in rewards between the winning and losing completions.
The first form, as in (source: Training language models to follow instructions with human feedback, 2022) and other works:
This formula represents the loss function as the negative logarithm of the sigmoid of the difference between the reward of the chosen response (yw) and the rejected response (yl) for a given prompt (x).
Second, as in (source: A general language assistant as a laboratory for alignment, 2021) and other works:
This formula provides an alternative loss function, expressed as the logarithm of one plus the exponentiated difference between the reward of the rejected response (yl) and the chosen response (yw) for a given prompt (x).

7.2 Architecture
The most common way reward models are implemented is through an abstraction similar to Transformer's AutoModelForSequenceClassification, which appends a small linear head to the language model that performs classification between two outcomes – chosen and rejected. At inference time, the model outputs the probability that the piece of text is chosen as a single logit from the model.
Other implementation options exist, such as just taking a linear layer directly from the final embeddings, but they are less common in open tooling.

7.3 Implementation Example
Implementing the reward modeling loss is quite simple. More of the implementation challenge is on setting up a separate data loader and inference pipeline. Given the correct dataloader, the loss is implemented as:
```python
import torch.nn as nn
rewards_chosen = model(**inputs_chosen)
rewards_rejected = model(**inputs_rejected)

loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()
```
Note, when training reward models, the most common practice is to train for only 1 epoch to avoid overfitting.

7.4 Variants
Reward modeling is a relatively under-explored area of RLHF. The traditional reward modeling loss has been modified in many popular works, but the modifications have not solidified into a single best practice.

7.4.1 Preference Margin Loss
In the case where annotators are providing either scores or rankings on a Likert Scale, the magnitude of the relational quantities can be used in training. The most common practice is to binarize the data direction, implicitly scores of 1 and 0, but the additional information has been used to improve model training. Llama 2 (source: Llama 2: Open foundation and fine-tuned chat models, 2023) proposes using the margin between two datapoints, m(r), to distinguish the magnitude of preference:
This formula shows the preference margin loss, modifying the standard loss by subtracting a margin term m(r) from the reward difference before applying the sigmoid and log functions.
Note that in Llama 3 the margin term was removed as the team observed diminishing improvements after scaling.

7.4.2 Balancing Multiple Comparisons Per Prompt
InstructGPT studies the impact of using a variable number of completions per prompt, yet balancing them in the reward model training (source: Training language models to follow instructions with human feedback, 2022). To do this, they weight the loss updates per comparison per prompt. At an implementation level, this can be done automatically by including all examples with the same prompt in the same training batch, naturally weighing the different pairs – not doing this caused overfitting to the prompts. The loss function becomes:
This formula represents the balanced loss function, averaging the standard negative log-sigmoid loss over all comparison pairs (yw, yl) drawn from the dataset D, weighted by the inverse of the total number of possible pairs (K choose 2).

7.4.3 K-wise Loss Function
There are many other formulations that can create suitable models of human preferences for RLHF. One such example, used in the popular, early RLHF'd models Starling 7B and 34B (source: Starling-7b: Improving helpfulness and harmlessness with rlaif, 2024), is a K-wise loss function based on the Plackett-Luce model (source: Learning plackett-luce mixtures from partial preferences, 2019).
Zhu et al. 2023 (source: Principled reinforcement learning with human feedback from pairwise or k-wise comparisons, 2023) formalizes the setup as follows. With a prompt, or state, s^i, K actions (a^i_0, a^i_1, ..., a^i_{K-1}) are sampled from P(a_0, ..., a_{K-1}|s^i). Then, labelers are used to rank preferences with sigma^i : [K] -> [K] is a function representing action rankings, where sigma^i (0) is the most preferred action. This yields a preference model capturing the following:
This formula defines the probability of observing a specific ranking (sigma^i) of K actions, given a state s^i, based on the Plackett-Luce model. It is calculated as the product, over ranks k from 0 to K-1, of the ratio between the exponentiated reward of the action ranked k-th and the sum of exponentiated rewards of all actions ranked k-th or lower.
When K = 2, this reduces to the Bradley-Terry (BT) model for pairwise comparisons. Regardless, once trained, these models are used similarly to other reward models during RLHF training.

7.5 Outcome Reward Models
The majority of preference tuning for language models and other AI systems is done with the Bradley Terry models discussed above. For reasoning heavy tasks, one can use an Outcome Reward Model (ORM). The training data for an ORM is constructed in a similar manner to standard preference tuning. Here, we have a problem statement or prompt, x and two completions y₁ and y2. The inductive bias used here is that one completion should be a correct solution to the problem and one incorrect, resulting in (yc, yic).
The shape of the models used is very similar to a standard reward model, with a linear layer appended to a model that can output a single logit (in the case of an RM) – with an ORM, the training objective that follows is slightly different (source: Training verifiers to solve math word problems, 2021):
[We] train verifiers with a joint objective where the model learns to label a model completion as correct or incorrect, in addition to the original language modeling objective. Architecturally, this means our verifiers are language models, with a small scalar head that outputs predictions on a per-token basis. We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model's final unembedding layer.
To translate, this is implemented as a language modeling head that can predict two classes per token (1 for correct, 0 for incorrect), rather than a classification head of a traditional RM that outputs one token for the entire sequence. Formally, following (source: Exploring the limit of outcome reward for learning mathematical reasoning, 2025) this can be shown as:
This formula represents the cross-entropy loss (L_CE) for training an Outcome Reward Model. It calculates the expected negative log-likelihood over the data distribution D, where 'r' is the binary label (1 for correct, 0 for incorrect) and 'p_theta(s)' is the model's predicted probability of correctness for sequence 's'.
where r ∈ 0,1 is a binary label where 1 applies to a correct answer to a given prompt and 0 applies to an incorrect, and p_theta(s) is the scalar proportional to predicted probability of correctness from the model being trained.
These models have continued in use, but are less supported in open-source RLHF tools. For example, the same type of ORM was used in the seminal work Let's Verify Step by Step (source: Let's verify step by step, 2023), but without the language modeling prediction piece of the loss. Then, the final loss is a cross entropy loss on every token predicting if the final answer is correct.
Given the lack of support, the term outcome reward model (ORM) has been used in multiple ways. Some literature, e.g. (source: Exploring the limit of outcome reward for learning mathematical reasoning, 2025), continues to use the original definition from Cobbe et al. 2021. Others do not.

7.6 Process Reward Models
Process Reward Models (PRMs), originally called Process-supervised Reward Models, are reward models trained to output scores at every step in a chain of thought reasoning process. These differ from a standard RM that outputs a score only at an EOS token or a ORM that outputs a score at every token. Process Reward Models require supervision at the end of each reasoning step, and then are trained similarly where the tokens in the step are trained to their relevant target - the target is the step in PRMs and the entire response for ORMs.
Here's an example of how this per-step label can be packaged in a trainer, from HuggingFace's TRL (source: TRL: Transformer reinforcement learning, 2020):
```python
# Get the ID of the separator token and add it to the completions
separator_ids = tokenizer.encode(step_separator, add_special_tokens=False)
completions_ids = [completion + separator_ids for completion in completions_ids]

# Create the label
labels = [[-100] * (len(completion) - 1) + [label] for completion, label in zip(completions_ids, labels)]
```
Traditionally PRMs are trained with a language modeling head that outputs a token only at the end of a reasoning step, e.g. at the token corresponding to a double new line or other special token. These predictions tend to be -1 for incorrect, 0 for neutral, and 1 for correct. These labels do not necessarily tie with whether or not the model is on the right path, but if the step is correct.

7.7 Reward Models vs. Outcome RMs vs. Process RMs vs. Value Functions
The various types of reward models covered indicate the spectrum of ways that "quality" can be measured in RLHF and other post-training methods. Below, a summary of what the models predict and how they are trained.

Table 3 presents a comparison of different types of reward models used in RLHF and related areas, outlining what they predict, how they are trained, and their typical language model (LM) structure.
- **Reward Models (Standard):** Predict the quality of text based on the probability of a response being chosen over another at the end-of-sequence (EOS) token. They are trained using contrastive loss (comparing pairs or groups of completions) and typically use a regression or classification head on top of LM features.
- **Outcome Reward Models (ORMs):** Predict the probability that an answer is correct on a per-token basis. Training relies on labeled outcome pairs (e.g., success/failure) in verifiable domains. The structure involves a language modeling head trained with per-token cross-entropy, where each label corresponds to the final outcome.
- **Process Reward Models (PRMs):** Predict a reward or score for intermediate steps within a reasoning process, specifically at the end of each step. Training uses intermediate feedback or stepwise annotations applied per token within the reasoning step. The LM structure includes a language modeling head that performs inference per reasoning step, often predicting three classes (e.g., -1, 0, 1 for incorrect, neutral, correct step).
- **Value Functions:** Predict the expected cumulative future return given the current state in a sequence. They are trained via regression, predicting a value for each point (token or state) in the sequence. The structure is often a classification head outputting a value per token.
The table highlights the different granularities and objectives these models address, from overall response preference (RM) to final correctness (ORM), step-by-step correctness (PRM), and expected future reward (Value Function).

Some notes, given the above table has a lot of edge cases.
*   Both in preference tuning and reasoning training, the value functions often have a discount factor of 1, which makes a value function even closer to an outcome reward model, but with a different training loss.
*   A process reward model can be supervised by doing rollouts from an intermediate state and collecting outcome data. This blends multiple ideas, but if the loss is per reasoning step labels, it is best referred to as a PRM.

7.8 Generative Reward Modeling
With the cost of preference data, a large research area emerged to use existing language models as a judge of human preferences or in other evaluation settings (source: Judging llm-as-a-judge with mt-bench and chatbot arena, 2023). The core idea is to prompt a language model with instructions on how to judge, a prompt, and two completions (much as would be done with human labelers). An example prompt, from one of the seminal works here for the chat evaluation MT-Bench (source: Judging llm-as-a-judge with mt-bench and chatbot arena, 2023), follows:
```
[System]
Please act as an impartial judge and evaluate the quality of the responses provided by two
AI assistants to the user question displayed below. You should choose the assistant that
follows the 'users instructions and answers the 'users question better. Your evaluation
should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
and level of detail of their responses. Begin your evaluation by comparing the two
responses and provide a short explanation. Avoid any position biases and ensure that the
order in which the responses were presented does not influence your decision. Do not allow
the length of the responses to influence your evaluation. Do not favor certain names of
the assistants. Be as objective as possible. After providing your explanation, output your
final verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]"
if assistant B is better, and "[[C]]" for a tie.
[User Question]
{question}
[The Start of Assistant 'As Answer]
{answer_a}
[The End of Assistant 'As Answer]
[The Start of Assistant 'Bs Answer]
{answer_b}
[The End of Assistant 'Bs Answer]
```
Given the efficacy of LLM-as-a-judge for evaluation, spawning many other evaluations such as AlpacaEval (source: Length-controlled alpacaeval: A simple way to debias automatic evaluators, 2024), Arena-Hard (source: From crowdsourced data to high-quality benchmarks: Arena-hard and BenchBuilder pipeline, 2024), and WildBench (source: WILDBENCH: Benchmarking LLMs with challenging tasks from real users in the wild, 2024), many began using LLM-as-a-judge instead of reward models to create and use preference data.
An entire field of study has emerged to study how to use so called "Generative Reward Models" (source: Generative reward models, 2024; source: Generative verifiers: Reward modeling as next-token prediction, 2024; source: Critique-out-loud reward models, 2024) (including models trained specifically to be effective judges (source: Prometheus: Inducing fine-grained evaluation capability in language models, 2023)), but on RM evaluations they tend to be behind existing reward models, showing that reward modeling is an important technique for current RLHF.
A common trick to improve the robustness of LLM-as-a-judge workflows is to use a sampling temperature of 0 to reduce variance of ratings.

7.9 Further Reading
The academic literature for reward modeling established itself in 2024. The bulk of progress in reward modeling early on has been in establishing benchmarks and identifying behavior modes. The first RM benchmark, RewardBench, provided common infrastructure for testing reward models (source: Rewardbench: Evaluating reward models for language modeling, 2024). Since then, RM evaluation has expanded to be similar to the types of evaluations available to general post-trained models, where some evaluations test the accuracy of prediction on domains with known true answers (source: Rewardbench: Evaluating reward models for language modeling, 2024) or those more similar to "vibes” performed with LLM-as-a-judge or correlations to other benchmarks (source: Rethinking reward model evaluation: Are we barking up the wrong tree?, 2024).
Examples of new benchmarks include multilingual reward bench (M-RewardBench) (source: M-RewardBench: Evaluating reward models in multilingual settings, 2024), RAG-RewardBench (source: RAG-RewardBench: Benchmarking reward models in retrieval augmented generation for preference alignment, 2024), RMB (source: RMB: Comprehensively benchmarking reward models in LLM alignment, 2024) or RM-Bench (source: RM-bench: Benchmarking reward models of language models with subtlety and style, 2024) for general chat, ReWordBench for typos (source: reWordBench: Benchmarking and improving the robustness of reward models with transformed inputs, 2025), MJ-Bench (source: MJ-bench: Is your multimodal reward model really a good judge for text-to-image generation?, 2024), Multimodal RewardBench (source: Multimodal rewardbench: Holistic evaluation of reward models for vision language models, 2025), VL RewardBench (source: VLRewardBench: A challenging benchmark for vision-language generative reward models, 2024), or VLRMBench (source: Vlrmbench: A comprehensive and challenging benchmark for vision-language reward models, 2025) for vision language models, Preference Proxy Evaluations (source: How to evaluate reward models for RLHF, 2024), and RewardMATH (source: Evaluating robustness of reward models for mathematical reasoning, 2024). Process reward models (PRMs) have their own emerging benchmarks, such as PRM Bench (source: PRMBench: A fine-grained and challenging benchmark for process-level reward models, 2025) and visual benchmarks of VisualProcessBench (source: VisualPRM: An effective process reward model for multimodal reasoning, 2025) and ViLBench (source: ViLBench: A suite for vision-language process reward modeling, 2025).
To understand progress on training reward models, one can reference new reward model training methods, with aspect-conditioned models (source: Interpretable preferences via multi-objective reward modeling and mixture-of-experts, 2024), high quality human datasets (source: HelpSteer2: Open-source dataset for training top-performing reward models, 2024; source: HelpSteer2-preference: Complementing ratings with preferences, 2024), scaling (source: Nemotron-4 340B technical report, 2024), extensive experimentation (source: Llama 2: Open foundation and fine-tuned chat models, 2023), or debiasing data (source: Offsetbias: Leveraging debiased data for tuning evaluators, 2024).