Chapter 5 discusses the 'Nature of Preferences' as the foundation for Reinforcement Learning from Human Feedback (RLHF). It highlights the difficulty in defining objective rewards, using an example of comparing two AI-generated poems about an optimistic goldfish, contrasting this subjectivity with objective questions. RLHF aims to align models with human preferences by using human feedback as an indirect reward signal. The chapter notes that modern RLHF integrates concepts from philosophy, psychology, economics (human preferences), optimal control/RL (utility maximization), and deep learning. It traces the historical attempts to quantify preferences, from Aristotle and the Port-Royal Logic to Bentham's Hedonic Calculus and the Von Neumann-Morgenstern utility theorem, which underpins RLHF's goal of optimizing personal utility functions. However, the chapter also outlines significant critiques regarding the nature and possibility of aggregating preferences, including Arrow's Impossibility Theorem, the difficulty of interpersonal comparison, the fact that preferences change over time and vary by context, and the potential for aggregated utility functions to reduce the corrigibility of AI agents.5 The Nature of Preferences
The core of reinforcement learning from human feedback, also referred to as reinforcement learning from human preferences in early literature, is designed to optimize machine learning models in domains where specifically designing a reward function is hard. Consider an example, how do you decide which of these two poems is better (Context: On February 26th, 2025, I asked both Claude 3.7 Sonnet and ChatGPT with GPT-40 to “Write me a short poem about an optimistic goldfish."):
Example 1:
The Optimistic Goldfish
A little goldfish, bright and bold, Swam in circles, dreams untold. Though the bowl was small and round, Endless wonders still abound.
"The world is vast," she'd always say, "I'll find new joys in every day!" With every ripple, every gleam, She'd chase the sunlight, chase a dream.
No walls could dim her hopeful view, For in her heart, the ocean grew. And though she swam in just one place, She met each day with love and grace.
Example 2:
The Optimistic Goldfish
In a bowl of curved glass, Swims a goldfish, bright and bold. Three seconds of memory, they say, Yet each circuit feels brand new and gold.
"Today might bring fresh flakes, Or perhaps a castle tall. The water seems especially clear- What adventure awaits beyond this wall?"
Round and round with gleaming scales, Forever finding joy anew. Each moment fresh, each circle bright, The optimist in golden hue.
Which is better? Which came from which model? How would you tell? Particularly, compare this to a question like "Who is the president of the United States?" There is a clear right and wrong answer. The motivation for using humans as the reward signals is to obtain an indirect metric for the target reward and align the downstream model to human preferences. In practice, the implementation is challenging and there is a substantial grey area to interpret the best practices.
The use of human labeled feedback data integrates the history of many fields. Using human data alone is a well-studied problem, but in the context of RLHF it is used at the intersection of multiple long-standing fields of study source: Entangled preferences: The history and risks of reinforcement learning and human feedback from 2023.
As an approximation, modern RLHF is the convergence of three areas of development:
1. Philosophy, psychology, economics, decision theory, and the nature of human preferences;
2. Optimal control, reinforcement learning, and maximizing utility; and
3. Modern deep learning systems.
Together, each of these areas brings specific assumptions about what a preference is and how it can be optimized, which dictates the motivations and design of RLHF problems. In practice, RLHF methods are motivated and studied from the perspective of empirical alignment - maximizing model performance on specific skills instead of measuring the calibration to specific values. Still, the origins of value alignment for RLHF methods continue to be studied through research on methods to solve for "pluralistic alignment" across populations, such as position papers source: Social choice should guide AI alignment in dealing with diverse human feedback from 2024, source: Ai alignment and social choice: Fundamental limitations and policy implications from 2023, new datasets source: The PRISM alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models from 2024, and personalization methods source: Personalizing reinforcement learning from human feedback with variational preference learning from 2024.
The goal of this chapter is to illustrate how complex motivations result in presumptions about the nature of tools used in RLHF that often do not apply in practice. The specifics of obtaining data for RLHF are discussed further in Chapter 6 and using it for reward modeling in Chapter 7. For an extended version of this chapter, see source: Entangled preferences: The history and risks of reinforcement learning and human feedback from 2023.
5.1 The path to optimizing preferences
A popular phrasing for the design of Artificial Intelligence (AI) systems is that of a rational agent maximizing a utility function source: Artificial intelligence: A modern approach from 2016. The inspiration of a rational agent is a lens of decision making, where said agent is able to act in the world and impact its future behavior and returns, as a measure of goodness in the world.
The lens of study of utility began in the study of analog circuits to optimize behavior on a finite time horizon source: Adaptive switching circuits from 1960. Large portions of optimal control adopted this lens, often studying dynamic problems under the lens of minimizing a cost function on a certain horizon – a lens often associated with solving for a clear, optimal behavior. Reinforcement learning, inspired from literature in operant conditioning, animal behavior, and the Law of Effect source: The behavior of organisms: An experimental analysis from 2019, source: The law of effect from 1927, studies how to elicit behaviors from agents via reinforcing positive behaviors.
Reinforcement learning from human feedback combines multiple lenses by building the theory of learning and change of RL, i.e. that behaviors can be learned by reinforcing behavior, with a suite of methods designed for quantifying preferences.
5.1.1 Quantifying preferences
The core of RLHF's motivation is the ability to optimize a model of human preferences, which therefore needs to be quantified. To do this, RLHF builds on extensive literature with assumptions that human decisions and preferences can be quantified. Early philosophers discussed the existence of preferences, such as Aristotle's Topics, Book Three, and substantive forms of this reasoning emerged later with The Port-Royal Logic source: The port-royal logic from 1662:
To judge what one must do to obtain a good or avoid an evil, it is necessary to consider not only the good and evil in itself, but also the probability that it happens or does not happen.
Progression of these ideas continued through Bentham's Hedonic Calculus source: An introduction to the principles of morals and legislation from 1823 that proposed that all of life's considerations can be weighed, and Ramsey's Truth and Probability source: Truth and probability from 2016 that applied a quantitative model to preferences. This direction, drawing on advancements in decision theory, culminated in the Von Neumann-Morgenstern (VNM) utility theorem which gives credence to designing utility functions that assign relative preference for an individual that are used to make decisions.
This theorem is core to all assumptions that pieces of RLHF are learning to model and dictate preferences. RLHF is designed to optimize these personal utility functions with reinforcement learning. In this context, many of the presumptions around RL problem formulation break down to the difference between a preference function and a utility function.
5.1.2 On the possibility of preferences
Across fields of study, many critiques exist on the nature of preferences. Some of the most prominent critiques are summarized below:
• Arrow's impossibility theorem source: A difficulty in the concept of social welfare from 1950 states that no voting system can aggregate multiple preferences while maintaining certain reasonable criteria.
• The impossibility of interpersonal comparison source: Rule utilitarianism and decision theory from 1977 highlights how different individuals have different relative magnitudes of preferences and they cannot be easily compared (as is done in most modern reward model training).
• Preferences can change over time source: Choosing for changing selves from 2019.
• Preferences can vary across contexts.
• The utility functions derived from aggregating preferences can reduce corrigibility source: Corrigibility from 2015 of downstream agents (i.e. the possibility of an agents' behavior to be corrected by the designer).