This chapter outlines the key developments in Reinforcement Learning from Human Feedback (RLHF). It traces the origins from early preference-based RL techniques like TAMER and COACH (pre-2018), highlighting Christiano et al.'s 2017 work applying RLHF to Atari trajectories as a pivotal moment. The chapter then discusses the period from 2019-2022, where RLHF was increasingly applied to large language models like GPT-2 and GPT-3 for tasks such as summarization, instruction following, and dialogue, alongside research into reward model over-optimization, alignment, and red teaming. Finally, it covers the current era (2023-Present), marked by ChatGPT's popularization of RLHF and its widespread adoption in models like Claude, Llama, and Nemotron. The field is evolving into broader preference fine-tuning (PreFT), encompassing methods like process rewards and Direct Preference Optimization (DPO).2 Key Related Works
In this chapter we detail the key papers and projects that got the RLHF field to where it is today. This is not intended to be a comprehensive review on RLHF and the related fields, but rather a starting point and retelling of how we got to today. It is intentionally focused on recent work that led to ChatGPT. There is substantial further work in the RL literature on learning from preferences (source: A survey of preference-based reinforcement learning methods from 2017). For a more exhaustive list, you should use a proper survey paper (source: A survey of reinforcement learning from human feedback from 2023), (source: Open problems and fundamental limitations of reinforcement learning from human feedback from 2023).
2.1 Origins to 2018: RL on Preferences
The field has recently been popularized with the growth of Deep Reinforcement Learning and has grown into a broader study of the applications of LLMs from many large technology companies. Still, many of the techniques used today are deeply related to core techniques from early literature on RL from preferences.
TAMER: Training an Agent Manually via Evaluative Reinforcement, Proposed a learned agent where humans provided scores on the actions taken iteratively to learn a reward model (source: Tamer: Training an agent manually via evaluative reinforcement from 2008). Other concurrent or soon after work proposed an actor-critic algorithm, COACH, where human feedback (both positive and negative) is used to tune the advantage function (source: Interactive learning from policy-dependent human feedback from 2017).
The primary reference, Christiano et al. 2017, is an application of RLHF applied to preferences between Atari trajectories (source: Deep reinforcement learning from human preferences from 2017). The work shows that humans choosing between trajectories can be more effective in some domains than directly interacting with the environment. This uses some clever conditions, but is impressive nonetheless. This method was expanded upon with more direct reward modeling (source: Reward learning from human preferences and demonstrations in atari from 2018). TAMER was adapted to deep learning with Deep TAMER just one year later (source: Deep tamer: Interactive agent shaping in high-dimensional state spaces from 2018).
This era began to transition as reward models as a general notion were proposed as a method for studying alignment, rather than just a tool for solving RL problems (source: Scalable agent alignment via reward modeling: A research direction from 2018).
2.2 2019 to 2022: RL from Human Preferences on Language Models
Reinforcement learning from human feedback, also referred to regularly as reinforcement learning from human preferences in its early days, was quickly adopted by AI labs increasingly turning to scaling large language models. A large portion of this work began between GPT-2, in 2018, and GPT-3, in 2020. The earliest work in 2019, Fine-Tuning Language Models from Human Preferences has many striking similarities to modern work on RLHF (source: Fine-tuning language models from human preferences from 2019). Learning reward models, KL distances, feedback diagrams, etc - just the evaluation tasks, and capabilities, were different. From here, RLHF was applied to a variety of tasks. The popular applications were the ones that worked at the time. Important examples include general summarization (source: Learning to summarize with human feedback from 2020), recursive summarization of books (source: Recursively summarizing books with human feedback from 2021), instruction following (InstructGPT) (source: Training language models to follow instructions with human feedback from 2022), browser-assisted question-answering (WebGPT) (source: Webgpt: Browser-assisted question-answering with human feedback from 2021), supporting answers with citations (GopherCite) (source: Teaching language models to support answers with verified quotes from 2022), and general dialogue (Sparrow) (source: Improving alignment of dialogue agents via targeted human judgements from 2022).
Aside from applications, a number of seminal papers defined key areas for the future of RLHF, including those on:
1. Reward model over-optimization (source: Scaling laws for reward model overoptimization from 2023): The ability for RL optimizers to over-fit to models trained on preference data,
2. Language models as a general area of study for alignment (source: A general language assistant as a laboratory for alignment from 2021), and
3. Red teaming (source: Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned from 2022) - the process of assessing safety of a language model.
Work continued on refining RLHF for application to chat models. Anthropic continued to use it extensively for early versions of Claude (source: Training a helpful and harmless assistant with reinforcement learning from human feedback from 2022) and early RLHF open-source tools emerged (source: Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization from 2022), (source: TrlX: A framework for large scale reinforcement learning from human feedback from 2023), (source: TRL: Transformer reinforcement learning from 2020).
2.3 2023 to Present: ChatGPT Era
The announcement of ChatGPT was very clear about the role of RLHF in its training (source: ChatGPT: Optimizing language models for dialogue from 2022):
We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup.
Since then RLHF has been used extensively in leading language models. It is well known to be used in Anthropic's Constitutional AI for Claude (source: Constitutional ai: Harmlessness from ai feedback from 2022), Meta's Llama 2 (source: Llama 2: Open foundation and fine-tuned chat models from 2023) and Llama 3 (source: The llama 3 herd of models from 2024), Nvidia's Nemotron (source: Nemotron-4 340B technical report from 2024), Ai2's TÃ¼lu 3 (source: T
 ULU 3: Pushing frontiers in open language model post-training from 2024), and more.
Today, RLHF is growing into a broader field of preference fine-tuning (PreFT), including new applications such as process reward for intermediate reasoning steps (source: Let's verify step by step from 2023), direct alignment algorithms inspired by Direct Preference Optimization (DPO) (source: Direct preference optimization: Your language model is secretly a reward model from 2024), learning from execution feedback from code or math (source: Training language models to self-correct via reinforcement learning from 2024), (source: Beyond human data: Scaling self-training for problem-solving with language models from 2023), and other online reasoning methods inspired by OpenAI's o1 (source: Introducing OpenAI ol-preview from 2024).