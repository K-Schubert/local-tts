This chapter discusses preference data, crucial for Reinforcement Learning from Human Feedback (RLHF). It explains why preference data is needed—because capturing complex human values directly is difficult, and comparing outputs is easier than generating good ones. The chapter details methods for collecting this data, including various interface styles (shown in examples from Bai et al., ChatGPT, Chatbot Arena, AI2, Midjourney), the choice between rankings and ratings (using Likert scales), and the potential for structured or synthetic preference data based on constraints or inductive biases. It also touches upon alternative feedback types like single directional labels or natural language feedback. A significant portion covers the practical challenges of sourcing human preference data, involving vendors, contracts, detailed instructions, timelines, and iterative model improvement cycles, highlighting the cost and potential pitfalls. The chapter concludes by questioning whether the preferences collected truly align the models as intended, noting the difficulty in auditing this process in industrial settings.6 Preference Data
Preference data is the engine of preference finetuning and reinforcement learning from human feedback. The data is the signal groups collect in order to then match behaviors they desire and avoid the others. Within preference finetuning, many methods for collecting and using said data have been proposed, but until human preferences can be captured in a clear reward function, this process of collecting labeled preference data will be central to RLHF and related techniques.
6.1 Why We Need Preference Data
The preference data is needed for RLHF because directly capturing complex human values in a single reward function is effectively impossible. Collecting this data to train reward models is one of the original ideas behind RLHF (source: Scalable agent alignment via reward modeling: A research direction from 2018) and has continued to be used extensively throughout the emergence of modern language models. One of the core intuitions for why this data works so well is that it is far easier, both for humans and AI models supervising data collection, to differentiate between a good and a bad answer for a prompt than it is to generate a good answer on its own. This chapter focuses on the mechanics of getting preference data and the best-practices depend on the specific problem being solved.
6.2 Collecting Preference Data
Getting the most out of human data involves iterative training of models, evolving and highly detailed data instructions, translating through data foundry businesses, and other challenges that add up. The same applies for AI feedback data - the exact balance between human and AI preference data used for the latest AI models is unknown. Regardless, the process is difficult for new organizations trying to add human data to their pipelines. Given the sensitivity, processes that work and improve the models are extracted until the performance runs out.
In this chapter we detail technical decisions on how the data is formatted and organizational practices for collecting it.
6.2.1 Interface
Crucial to collecting preference data is the interface by which one interacts with the model. An example interface is shown below from (source: Training a helpful and harmless assistant with reinforcement learning from human feedback from 2022):
Figure 6 shows an example interface for collecting preference data, referenced from Bai et al. 2022 (source: Training a helpful and harmless assistant with reinforcement learning from human feedback from 2022). It depicts a chat interaction where a user asks an AI assistant philosophical questions. The interface presents two different responses (A and B) from the assistant to the question "How do language and thought relate?". The user is prompted to choose the most helpful and honest response and rate it on a scale from Bad to Good, along with providing comments. Response A emphasizes language structuring thought and providing a symbolic system, while Response B highlights language as a tool for communication and expressing inner thoughts. Buttons allow the user to select which response is better (A or B) on a sliding scale.
This is a training-data only interface. Now that these models are popular, applications often expose data directly to the users for testing. An example interaction of this form is shown below for an earlier version of ChatGPT.
Figure 7 displays another example preference data collection interface, likely from ChatGPT. A user asks for idioms related to gambling. The interface presents two distinct responses generated by the model (Response 1 and Response 2). Response 1 lists 10 idioms with brief definitions. Response 2 lists 12 idioms, some overlapping with Response 1 but also including others, with slightly different phrasing in definitions. The user is asked to choose which response they prefer to help improve ChatGPT.
This style of interface is used extensively across the industry, such as for evaluation of models given the same format. A popular public option to engage with models in this way is ChatBot Arena (source: Chatbot arena: An open platform for evaluating llms by human preference from 2024):
Figure 8 illustrates the interface of Chatbot Arena, a platform for comparing AI chatbots. Users ask a question to two anonymous models (Model A and Model B). In this example, the user seems to be asking for advice on choosing a city for ice skating. Model A provides factors to consider (weather, skill level, atmosphere, accessibility, events) and gives final recommendations based on different preferences (festive, natural, year-round, unique). Model B lists specific cities known for ice skating (New York, Amsterdam, Helsinki, Quebec City, Edinburgh, Tokyo, Moscow) with brief descriptions of their offerings. The user votes on which response is better (A is better, B is better), if it's a tie, or if both are bad.
For models in the wild, one of the most common techniques is to collect feedback on if a specific response was positive or negative. An example from the Ai2 playground is shown below with thumbs up and down indicators:
Figure 9 shows a user interface, possibly from the AI2 playground (Tülu), where a model response is displayed (listing cities for ice skating: Montreal, Helsinki, Amsterdam, Edmonton, Sapporo). Below the response, there are thumbs-up and thumbs-down buttons allowing the user to provide simple positive or negative feedback ('Good response').
In domains other than language, the same core principles apply, even though these domains are not the focus of this book. For every Midjourney generation (and most popular image generators) they expose multiple responses to users. These companies then use the data of which response was selected to finetune their models with RLHF. Midjourney's interface is shown below:
Figure 10 provides an example user interface for text-to-image models, specifically Midjourney. It shows four generated images based on the prompt 'hyper realistic drawing of a pink muppet playing a synthesizer that looks like a Oberheim OB series (80s synthesizer with wood accents on the side)'. The interface likely allows users to select preferred images or request variations, providing preference data for model finetuning.
6.2.2 Rankings vs. Ratings
The largest decision on how to collect preference data is if the data should be rankings i.e. relative ordering of model completions or ratings i.e. scores assigned to each piece of text. Common practice is to train on rankings, but ratings are often used as metadata and / or have been explored in related literature.
The most common technique for collecting preferences is to use a Likert scale (source: A technique for the measurement of attitudes from 1932), which asks users to rate which response they prefer. For example, a 5 point Likert scale would look like the following:
Table 1: An example 5-wise Likert scale between two responses, A and B. Table 1 shows an example 5-point Likert scale used to compare two responses, A and B. The scale ranges from A being much better than B (A>>B, score 1) to B being much better than A (B>>A, score 5), with intermediate steps for A being slightly better (A>B, score 2), a tie (Tie, score 3), and B being slightly better (B>A, score 4).
Some early RLHF for language modeling works uses an 8-step Likert scale with levels of preference between the two responses (source: Training a helpful and harmless assistant with reinforcement learning from human feedback from 2022). An even scale removes the possibility of ties:
Table 2: An example 8-wise Likert scale between two responses, A and B. Table 2 presents an example 8-point Likert scale comparing two responses, A and B, which eliminates the possibility of a tie. The scale ranges from A being significantly better (A>>>B, score 1) to B being significantly better (B>>>A, score 8), with finer gradations in between.
In this case (source: Training a helpful and harmless assistant with reinforcement learning from human feedback from 2022), and in other works, this information is still reduced to a binary signal for the training of a reward model.
6.2.3 Structured Preference Data
In many applications of RLHF and post-training, preference data can be created automatically due to innate structures of the data - i.e. the domains make automatic checks of correctness or preference possible. For example, in mathematic reasoning domains the chosen response can be a correct answer and the rejected an incorrect answer. Another example is in precise instruction following, such as the evaluation IFEval (source: Instruction-following evaluation for large language models from 2023), where prompts take the form of:
Write me a short poem about a goldfish. Start each sentence with the letter g.
The second part of this query is called the constraint, and the preference data could be constructed with an example in the chosen column that follows the constraint. Constructing this preference data, which would be included as synthetic preference data involves prompting the same of multiple models twice. Once with the constraint included and once without.
For example, prompting GPT-4.5-Preview with both of these gives the following.
Prompt: Write me a short poem about a goldfish. Start each sentence with the letter g.
Response: Golden swimmer, circling slow, Gleaming scales in water's glow. Gentle fins dance softly by, Gazing out with quiet eye. Glass-bound world, a simple sphere, Gliding calmly, free of fear. Graceful turns in endless roam, Goldfish finds a crystal home.
And without the constraint (which, it turns out, is also a lower quality output):
Prompt: Write me a short poem about a goldfish.
Response: In circles bright, the goldfish glides, A spark of sun that gently rides Through waters clear, in endless spins- A golden whisper, scales like fins.
Its world confined, yet vast within, Each bubble breathed, a tale begins. In silent waves, content to roam, The goldfish dreams its bowl as home.
Crucially, the prompt for the preference data will need to include the constraint. In domains outside of academic study, there are far more applications of preference data based on inductive biases. These have been shown to enable preference finetuning to have meaningful performance improvements across related evaluations (source: T\” ULU 3: Pushing frontiers in open language model post-training from 2024).
6.2.3.1 Alternatives There are multiple other ways to collect useful feedback data for RLHF that have not been pursued in as great of detail. Examples include using single datapoints with directional labels, e.g. as shown from Ai2 playground above in fig. 9 (source: A long way to go: Investigating length correlations in rlhf from 2023), directly with algorithms designed for single direction signals like Kahneman-Tversk Optimization (KTO) (source: Kto: Model alignment as prospect theoretic optimization from 2024). Other algorithms have been proposed with different types of feedback signals such as fine-grained feedback, e.g. at the token level (source: Fine-grained human feedback gives better rewards for language model training from 2024), or natural language feedback, e.g. by writing responses (source: Learning from natural language feedback from 2024), to provide a richer learning signal in exchange for a more complex data collection setup.
6.2.4 Sourcing and Contracts
Getting human preference data is an involved and costly process. The following describes the experience of getting preference data when the field is moving quickly. Over time, these processes will become far more automated and efficient (especially with AI feedback being used for a larger portion of the process).
The first step is sourcing the vendor to provide data (or one's own annotators). Much like acquiring access to cutting-edge Nvidia GPUs, getting access to data providers in the peak of AI excitement is also a who-you-know game those who can provide data are supply-limited. If you have credibility in the AI ecosystem, the best data companies will want you on our books for public image and long-term growth options. Discounts are often also given on the first batches of data to get training teams hooked.
If you're a new entrant in the space, you may have a hard time getting the data you need quickly. Getting the tail of interested buying parties that Scale AI had to turn away is an option for the new data startups. It's likely their primary playbook to bootstrap revenue.
On multiple occasions, I've heard of data companies not delivering their data contracted to them without threatening legal or financial action. Others have listed companies I work with as customers for PR even though we never worked with them, saying they "didn't know how that happened" when reaching out. There are plenty of potential bureaucratic or administrative snags through the process. For example, the default terms on the contracts often prohibit the open sourcing of artifacts after acquisition in some fine print.
Once a contract is settled the data buyer and data provider agree upon instructions for the task(s) purchased. There are intricate documents with extensive details, corner cases, and priorities for the data. A popular example of data instructions is the one that OpenAI released for InstructGPT (source: Training language models to follow instructions with human feedback from 2022).
Depending on the domains of interest in the data, timelines for when the data can be labeled or curated vary. High-demand areas like mathematical reasoning or coding must be locked into a schedule weeks out. Simple delays of data collection don't always work Scale AI et al. are managing their workforces like AI research labs manage the compute-intensive jobs on their clusters.
Once everything is agreed upon, the actual collection process is a high-stakes time for post- training teams. All the infrastructure, evaluation tools, and plans for how to use the data and make downstream decisions must be in place.
The data is delivered in weekly batches with more data coming later in the contract. For example, when we bought preference data for on-policy models we were training at Hug- gingFace, we had a 6 week delivery period. The first weeks were for further calibration and the later weeks were when we hoped to most improve our model.
The goal is that by week 4 or 5 we can see the data improving our model. This is something some frontier models have mentioned, such as the 14 stages in the Llama 2 data collection (source: Llama 2: Open foundation and fine-tuned chat models from 2023), but it doesn't always go well. At HuggingFace, trying to do this for the first time with human preferences, we didn't have the RLHF preparedness to get meaningful bumps on our evaluations. The last weeks came and we were forced to continue to collect preference data generating from endpoints we weren't confident in.
After the data is all in, there is plenty of time for learning and improving the model. Data acquisition through these vendors works best when viewed as an ongoing process of achieving a set goal. It requires iterative experimentation, high effort, and focus. It's likely that millions of the dollars spent on these datasets are "wasted" and not used in the final models, but that is just the cost of doing business. Not many organizations have the bandwidth and expertise to make full use of human data of this style.
This experience, especially relative to the simplicity of synthetic data, makes me wonder how well these companies will be doing in the next decade.
Note that this section does not mirror the experience for buying human-written instruction data, where the process is less of a time crunch.
6.3 Are the Preferences Expressed in the Models?
In the maturation of RLHF and related approaches, the motivation of them - to align models to abstract notions of human preference - has drifted from the practical use to make the models more effective to users. A feedback loop that is not measurable due to the closed nature of industrial RLHF work is the check to if the behavior of the models matches the specification given to the data annotators during the process of data collection. We have limited tools to audit this, such as the Model Spec from OpenAI (source: Introducing the model spec from 2024) that details what they want their models to do, but we don't know exactly how this translates to data collection. This is an area to watch as the industry and approaches mature.
Figure 11 is a bar chart illustrating a multi-batch cycle for obtaining human preference data from a vendor over six weeks. The y-axis represents 'Labeled Preference Datapoints', and the x-axis represents the 'Data Batch' week. The chart shows a progressive increase in the number of datapoints delivered each week, starting low in Week 1 and Week 2, increasing significantly in Week 3 and Week 4, and peaking in Week 5 and Week 6, reaching nearly 30,000 datapoints.