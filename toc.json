{
    "sections": "{ \"1 Introduction\": [5, 12], \"2 Key Related Works\": [13, 14], \"3 Definitions & Background\": [15, 18], \"4 Training Overview\": [19, 22], \"5 The Nature of Preferences\": [23, 25], \"6 Preference Data\": [26, 35], \"7 Reward Modeling\": [36, 42], \"8 Regularization\": [43, 45], \"9 Instruction Finetuning\": [46, 48], \"10 Rejection Sampling\": [49, 53], \"11 Policy Gradient Algorithms\": [54, 72], \"12 Direct Alignment Algorithms\": [73, 81], \"13 Constitutional AI & AI Feedback\": [82, 83], \"14 Reasoning Training & Inference-Time Scaling\": [84, 88], \"15 Synthetic Data & Distillation\": [89, 90], \"16 Evaluation\": [91, 97], \"17 Over Optimization\": [98, 103], \"18 Style and Information\": [104, 106], \"19 Product, UX, and Model Character\": [107, 109], \"Bibliography\": [110, \"NA\"] }"
}